Technical Design Document (TDD) – Plan Mode Integration

Architecture Overview:
This extension builds on the existing Claude Code Critic architecture where Plan Mode is fully implemented (per MILESTONES.md completion). In the current architecture, when a user enters Plan Mode (e.g. pressing Shift+Tab twice in the CLI or via our GitHub Action’s non-interactive plan step), Claude uses its built-in Plan sub-agent for codebase exploration ￼ ￼. The Plan agent executes only read-only tools (Glob, Grep, Read, etc.) and compiles a plan.md which is then presented for approval ￼ ￼. Our design will hook into this process at two points:
	1.	During plan generation (prompt level) – injecting additional instructions and ensuring the Plan sub-agent has access to needed context via tools.
	2.	After plan generation (post-processing) – verifying or refining the plan before it’s shown to the user, to enforce the quality rules.

The enhancements will be implemented as follows:

1. Extending Plan Agent Prompt & Tools:
	•	Prompt Augmentation: We will modify the system prompt or initial user prompt when entering Plan Mode to include our custom “planning guidelines.” Technically, this can be done via a .claude/agents/plan.md override if the tool allows (though Plan is built-in, we might simulate it by a preamble in the conversation). Another approach is using a User Prompt Hook – for example, intercept the user’s request to plan and prepend a note to Claude. Claude Code Critic could detect when Plan Mode is activated (via internal state or the Plan Mode toggle command) and then programmatically prepend something like:
“NOTE: When formulating the plan, ensure you (1) search for existing code that might fulfill the requirements to avoid duplication, (2) include tasks for writing tests or docs if relevant, (3) adhere to our architecture and note any potential performance or design concerns.”
This message can be given a high priority (possibly as a system-level instruction if supported, or a high-level user message just after entering Plan Mode). By embedding phrases like “You MUST include tests if applicable” or “Do NOT reinvent existing code – reuse if possible,” we leverage Claude’s compliance with explicit instructions ￼. We will verify via testing that Claude indeed incorporates these instructions in the plan output.
	•	MCP Tooling: We set up MCP endpoints to provide rich context. For example, if we have an internal documentation server or an index service, we register it as an MCP tool (e.g. DocSearch or IndexQuery). In our Claude Code Critic config (~/.claude/config.yaml or similar), under mcpServers we will list our context server, and under allowedTools for the Plan agent, include WebSearch/WebFetch or a custom QueryDocs tool if needed ￼. For instance, if the plan needs to fetch a ticket’s acceptance criteria (like the Shortcut example ￼), it could call WebFetch on a known URL or a custom RPC.
Concretely, we might implement an MCP server endpoint /search_index?q=... that returns relevant content (design docs, ADRs, etc.) as text. The Plan agent can then use WebSearch/WebFetch (with the query) to retrieve that content into context. This requires configuration: in Claude Code’s config, ensure the Plan sub-agent is permitted to use those tools. According to Anthropic’s docs, Plan subagent has Grep, Glob, Read, Bash by default ￼. We may need to extend its permissions slightly. One strategy is to allow WebSearch in plan mode for our specific use-case. Alternatively, use the Bash tool in read-only mode to call a local script that queries the index (since Bash is available even in plan mode, but restricted to read-only commands ￼). We could, for example, allow a safe command like curl http://localhost:5000/search_index?query="..."" via Bash. This would return data that the Plan agent can Read. This is a bit hacky but works within the allowed tools. We’ll choose the method that’s simplest and secure – likely whitelisting a specific MCP search.
	•	Parallel Sub-agents: Plan Mode already spawns multiple internal sub-agents in parallel for exploration ￼. We don’t need to implement multi-threading ourselves; we rely on Claude’s built-in behavior. However, we do want to ensure that one of the perspectives includes looking for tests/docs. We can do this by phrasing part of the request such that Claude launches a sub-search for related tests. For example, if the user’s request is “Add feature X to module Y,” our augmented prompt could include, “(Remind: check if module Y has tests or relevant documentation.)”. In practice, Claude might then use one Explore agent to grep for module Y references in /tests directory. Another might grep the codebase for keywords related to the feature (ensuring it’s not duplicate). Claude’s planning algorithm will merge these findings. Since the architecture (Opus 4.5, etc.) supports summarizing each sub-agent’s results ￼, our instructions will effectively guarantee one summary is about tests/docs if applicable. We might also utilize the Todo tool that plan mode provides (Claude can list tasks to itself). But direct prompting is simpler.
	•	Persistent Plan & CLAUDE.md: If MILESTONES.md introduced persistent plan storage ￼ ￼, our changes don’t conflict with that. In fact, since plans are saved, any extra tasks we add (like tests, docs) will be recorded for future reference. We should also update CLAUDE.md (the knowledge file) with any new project-specific rules, e.g. “All new features must have accompanying tests. Do not duplicate functionality – search existing modules first.” Claude Code reads CLAUDE.md for general guidance, which will reinforce our runtime instructions. This is a passive but powerful way to align with our standards ￼.

2. Plan Quality Post-Processing:
	•	Automatic Plan QA: Once Claude generates the plan (as markdown text), Claude Code Critic’s workflow can intercept before final output. Ideally, we want Claude itself to handle most adjustments via the prompt, but as a safety net, we implement a post-hoc checker in our tool. For example, after receiving the plan.md content, we can write a small Python script or function to analyze it:
	•	Parse the markdown tasks (e.g. tasks might be listed as a checklist - [ ] Task description in the plan). Identify if any tasks mention “test” or “doc”. If the feature clearly warrants tests (we can guess by code files or user story), but none found, we flag it. How to guess? We could scan the diff or user request – e.g. if adding a new API or module, likely needs tests. Or simpler: see if any existing test files relate to the changed area (we have the code index, could search for moduleY.test during planning as we said). If such evidence exists and the plan lacks a test task, that’s a gap.
	•	Similarly, ensure at least one task for docs if the change is user-facing (harder to infer automatically, but if the module is in a docs/ or if user explicitly mentions documentation, etc.).
	•	Search for known duplicate indicators: e.g. if plan has a task “Create new function Z” and our codebase actually has function Z or similar functionality, it’s a red flag. We can programmatically grep the codebase for function Z definition. However, since Claude likely would have found it if it existed (given our prompt), this is just a verification. Another heuristic: building a simple index of all function/class names in the project (maybe available from CLAUDE.md summary) and checking if any new names in plan match existing names.
	•	Check architecture consistency: This is more semantic. We might rely on CLAUDE.md patterns or a list of approved design patterns. Automated check could be minimal (like ensure the plan doesn’t suggest modifying layers it shouldn’t, e.g. UI code in a backend plan, etc. – but that’s highly project-specific).
	•	Feedback Loop: If our post-processor finds any issues, we have a few options:
	1.	Automated revision: We could programmatically insert an extra step into the plan (e.g. append “- [ ] Write unit tests for X” if missing). However, altering Claude’s output might confuse its internal state. It might be safer to re-prompt Claude with the findings: e.g. provide the plan back to Claude with feedback “You forgot tests for module Y – update the plan to include that.” This would require going through another Claude message cycle, which could be done if we orchestrate it. But it may complicate the flow and cost more tokens.
	2.	Highlight to user: Alternatively, we present the plan as-is to the user but with a warning or annotation in the UI (outside of Claude’s output) for any detected omissions. For instance, the CLI or GitHub comment could include: “Note: No test tasks included – consider adding tests.” However, since our aim is to automate this, having the AI include it is preferable.
A balanced approach: use prompt instructions strongly so that the plan is usually correct, and limit post-processing to printing a gentle reminder if something obvious is missing. We will implement the detection but perhaps not auto-edit the plan except for very straightforward additions. The user can always edit the plan file (Claude encourages that in Plan Mode ￼ ￼), so showing a suggestion might suffice.
	•	Testing the QA: We will test scenarios where we intentionally ask for a change with known pitfalls. For example, “Add a new setting to config” – this should ideally trigger a documentation update (like update README). If not, our checker would catch it and maybe prompt Claude again. We’ll iterate until the common cases are handled.

3. Integration with Execution Phase:
	•	Once a plan is approved, execution proceeds with the general-purpose agent (which can write files) ￼. Our plan improvements don’t directly change execution, but they influence it (execution will follow the refined plan). We should ensure that any Plan steps that involve tests or docs can indeed be executed. For instance, if the plan says “Update docs in README”, the subsequent execution agent should be able to open that file and modify it. Claude Code’s general agent can handle that (assuming docs are in the repo and not read-only).
	•	If using sub-agents during execution (like test-runner sub-agent – more on that in Feature 2), the plan might instruct to run tests. We should verify that Claude can indeed call the appropriate sub-agent or tool. For example, if a plan step is “Run tests to confirm nothing breaks,” Claude might use the test-runner sub-agent (if we create one) or just execute npm test via Bash. We should be prepared for that (e.g. ensure the test environment is set up in CI or the local environment).
	•	No changes are needed to how the plan persists on disk except possibly adding extra metadata. Given Plan Mode now persists plans to ~/.claude/plans/ ￼, our additional tasks will just be part of that markdown. If we added annotations like “(auto-checked for duplicates)” etc., that’s fine to store.

4. Tools and Config Adjustments:
	•	In the Claude Code Critic repository, update any relevant configuration:
	•	CLAUDE.md: Add sections about avoiding duplication and including tests/docs (if not already in project guidelines).
	•	Plan Mode Toggle Hook: If our system allows hooking on entering Plan Mode (some community solutions do, as seen on Reddit with a UserPromptSubmit hook ￼), implement a hook function that appends our instructions. For example, inside our CLI or action, detect the @agent-plan usage or Plan Mode command and then call our augmentation logic.
	•	MCP Server: Implement a simple MCP server or use an existing one to serve documentation. If none exists, this could be as simple as enabling WebSearch to Google our internal wiki (not ideal for private info) or better, host a local file server that returns specific docs. We will likely create a small Node/Python server that reads a directory of markdown specs/PRDs and returns matches. The Claude Code Critic Action (if running in GitHub Actions) can run this server in parallel so that Claude’s WebFetch to http://127.0.0.1:PORT/docsearch?query=... works. Security: That server will be closed after, and since it’s local in the Action environment, no external exposure.
	•	Allowed Tools: In .claude-project or CLI config, ensure WebFetch (or the chosen method) is listed under allowed tools in Plan Mode. Possibly need to update Plan agent’s YAML if it’s customizable (though built-in, maybe not easily). If direct editing of built-in isn’t possible, we rely on Bash tool as described.

5. Example Workflow with Enhancements:
Consider a user story: “Add a new user role ‘Moderator’ with specific permissions.” The developer triggers Plan Mode for this task. Under the hood:
	•	Claude’s Plan agent starts and receives an augmented prompt: it knows to search for existing roles and permission code (to avoid duplicating logic), check for any mention of this feature in documentation or CLAUDE.md (maybe a design decision), and plan for tests/docs.
	•	Claude spawns subagents in parallel: one searches src/ for “role” implementations (finding perhaps Admin and User roles), another searches docs/ for “roles” (via MCP, finds maybe an ADR about roles), another looks in tests/ for permission tests. Each returns info.
	•	The Plan agent integrates this: it finds in code that roles are defined in a config file and maybe an enum. It finds in docs that new roles require updating the admin guide. So the plan it composes might be:
	1.	Modify roles enum to add “Moderator”.
	2.	Update permission checks in functions X, Y to include Moderator logic.
	3.	Add unit tests for new Moderator permissions in permissions.test.js (none exist yet, so create).
	4.	Update documentation (Security.md) to describe the new role.
	5.	Ensure no duplicate logic: uses same pattern as existing roles.
	6.	(Maybe prompt user if any unclear requirement from ADR – but that’s extra).
	•	Our post-check sees this plan is pretty good: it included tests/docs. It might notice “Ensure no duplicate logic” is a note, which is great. If something was missing, we’d catch it.
	•	The plan is presented. The user approves. In execution, Claude follows these steps. When it reaches “Add unit tests,” if we have a test-runner subagent (next feature), it might automatically run them. If not, it will write test code and then perhaps run them via Bash.

This example shows how the enriched planning phase leads to a complete implementation plan that aligns with quality standards.

6. Edge Cases & Considerations:
	•	Trivial tasks: We should ensure our guardrails don’t overkill on trivial changes. Plan Mode itself is usually skipped for trivial changes ￼, but if used, we don’t want a one-line bugfix plan to always say “write tests” when maybe not needed. We rely on Claude’s judgment (and our CLAUDE.md guidance can say e.g. “small fixes may not need new tests unless they fix a bug that lacked coverage”). We can also add logic: if the plan is extremely small or marked as trivial, maybe the post-check doesn’t enforce test/doc steps strictly.
	•	False duplications: The check for duplicate functionality must be careful. If a feature is genuinely new, there won’t be existing code. We must ensure the plan doesn’t get stuck searching endlessly for something that’s not there. Our prompt should say “if none found, proceed with new implementation.” Claude’s search subagents have thoroughness levels (quick, medium, thorough) ￼ – we might default to “medium” thoroughness in plan mode to balance speed and coverage.
	•	Plan length and readability: By adding tasks, plans might become longer. That’s expected, but we should keep them structured and clear (Claude does output nice markdown checklists). The user can always edit the plan file if it’s too verbose.
	•	Claude’s token limits: More instructions and context mean more tokens. However, thanks to auto-compact and Opus’s improved efficiency in the latest version ￼ ￼, this should be manageable. We will still monitor that the context (including any fetched docs) doesn’t overflow. If needed, instruct Claude to summarize fetched external docs rather than include huge raw text.

References & Rationale:
	•	This design leverages known capabilities of Claude Code’s Plan Mode, such as parallel exploration with subagents ￼ ￼ and structured plan outputs with persistent files ￼. By integrating quality checks early, we mimic a senior engineer’s planning process where they consider testing and avoid redundant work upfront ￼.
	•	Code review best practices were considered: e.g., “Ensure the changes are consistent with the existing system architecture and do not duplicate existing functionality” ￼ directly influenced our duplication check requirement.
	•	The notion of including tests/documentation stems from standard development workflows and the Kier Eagan Manifesto joke in the blog (virtues of production include “Vigilance in testing” and the refinement principles include documentation ￼ ￼). We operationalize those principles by baking them into the AI’s plan.
	•	Overall, this deeper plan integration ensures the AI doesn’t just generate code quickly, but does so thoughtfully, aligning with the trend that “Plan Mode systematically improves code quality by enforcing the planning phase” ￼.
